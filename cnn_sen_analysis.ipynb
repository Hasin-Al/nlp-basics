{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcdDW36N_kU3",
        "outputId": "01c9c16a-9a44-423d-be02-6ce9a5d4af8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8XMcmrHMfs3"
      },
      "source": [
        "# create a vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjHWO_z9JNcV"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "#nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsNzveWIJ8H9",
        "outputId": "6d608911-0fda-4031-b170-bb3b39e2d1a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnYV2N3rC_xj"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_lrHAwoD_A8"
      },
      "outputs": [],
      "source": [
        "#functions\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "# read all text\n",
        "  text = file.read()\n",
        "# close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  return tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "# load doc\n",
        "  doc = load_doc(filename)\n",
        "\n",
        "# clean doc\n",
        "  tokens = clean_doc(doc)\n",
        "# update counts\n",
        "  vocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "# walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "    if filename.startswith('cv9'):\n",
        "      continue\n",
        "# create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "# add doc to vocab\n",
        "    add_doc_to_vocab(path, vocab)\n",
        "\n",
        "def save_list(lines, filename):\n",
        "# convert lines to a single blob of text\n",
        "  data = '\\n'.join(lines)\n",
        "# open file\n",
        "  file = open(filename, 'w')\n",
        "# write text\n",
        "  file.write(data)\n",
        "# close file\n",
        "  file.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX6pC7qnG2tb",
        "outputId": "3d11d0a9-6567-42c3-842f-d79eb8eee6b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44276\n"
          ]
        }
      ],
      "source": [
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "#txt_sentoken = '/content/drive/MyDrive/txt_sentoken'\n",
        "process_docs('/content/drive/MyDrive/txt_sentoken/pos', vocab)\n",
        "process_docs('/content/drive/MyDrive/txt_sentoken/neg', vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmMzTxcSHzt5"
      },
      "outputs": [],
      "source": [
        "#vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y85OlM9KK2Qu",
        "outputId": "aefb50eb-ffb6-4f67-b369-5a8b549ecb7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25767\n"
          ]
        }
      ],
      "source": [
        "# keep tokens with a min occurrence\n",
        "min_occurane = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, '/content/drive/MyDrive/txt_sentoken/vocab.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rdabx6y1LQCL"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbsxFbNozmua"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5hlZP3zMZJJ"
      },
      "source": [
        "# Train a CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qR7KP6N5PK0V"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esHUfVkwFcm-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvSgjZQJLR3D"
      },
      "outputs": [],
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "# split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# filter out tokens not in vocab\n",
        "  tokens = [w for w in tokens if w in vocab]\n",
        "  tokens = ' '.join(tokens)\n",
        "  return tokens\n",
        "\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "  documents = list()\n",
        "# walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "# create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "# load the doc\n",
        "    doc = load_doc(path)\n",
        "# clean doc\n",
        "    tokens = clean_doc(doc, vocab)\n",
        "# add to list\n",
        "    documents.append(tokens)\n",
        "  return documents\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "# load documents\n",
        "  neg = process_docs('/content/drive/MyDrive/txt_sentoken/neg', vocab, is_train)\n",
        "  pos = process_docs('/content/drive/MyDrive/txt_sentoken/pos', vocab, is_train)\n",
        "  docs = neg + pos\n",
        "# prepare labels\n",
        "  labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "  return docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "# integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(docs)\n",
        "# pad sequences\n",
        "  padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "  return padded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVxtyJRNMV2F"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "  model.add(Conv1D(filters=32, kernel_size=12, activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "# compile network\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# summarize defined model\n",
        "  model.summary()\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKVqouvsQJtd"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4GHDZXDQJwQ",
        "outputId": "63fe93c2-165b-4a58-9afd-c9126816dcd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 25768\n",
            "Maximum length: 1317\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 1317, 100)         2576800   \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 1306, 32)          38432     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 653, 32)          0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 20896)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                208970    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,824,213\n",
            "Trainable params: 2,824,213\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "57/57 - 27s - loss: 0.6883 - accuracy: 0.5394 - 27s/epoch - 481ms/step\n",
            "Epoch 2/10\n",
            "57/57 - 26s - loss: 0.5059 - accuracy: 0.7450 - 26s/epoch - 465ms/step\n",
            "Epoch 3/10\n",
            "57/57 - 26s - loss: 0.1211 - accuracy: 0.9544 - 26s/epoch - 463ms/step\n",
            "Epoch 4/10\n",
            "57/57 - 26s - loss: 0.0121 - accuracy: 0.9983 - 26s/epoch - 463ms/step\n",
            "Epoch 5/10\n",
            "57/57 - 24s - loss: 0.0024 - accuracy: 1.0000 - 24s/epoch - 424ms/step\n",
            "Epoch 6/10\n",
            "57/57 - 27s - loss: 0.0014 - accuracy: 1.0000 - 27s/epoch - 465ms/step\n",
            "Epoch 7/10\n",
            "57/57 - 26s - loss: 0.0010 - accuracy: 1.0000 - 26s/epoch - 463ms/step\n",
            "Epoch 8/10\n",
            "57/57 - 27s - loss: 8.1359e-04 - accuracy: 1.0000 - 27s/epoch - 468ms/step\n",
            "Epoch 9/10\n",
            "57/57 - 26s - loss: 6.8115e-04 - accuracy: 1.0000 - 26s/epoch - 448ms/step\n",
            "Epoch 10/10\n",
            "57/57 - 25s - loss: 5.8914e-04 - accuracy: 1.0000 - 25s/epoch - 444ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1fcc5678e0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = '/content/drive/MyDrive/txt_sentoken/vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)\n",
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "# define model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# save the model\n",
        "#model.save('model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0_I-of3QJzA",
        "outputId": "a6eca84c-fedf-4585-82ca-7ad5ad00cf1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  30, 2806,  325, ...,    0,    0,    0], dtype=int32)"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rQxf4LSQJ03"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOQFBZg4McbE"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyWYdw88MV4f"
      },
      "outputs": [],
      "source": [
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
        "# clean review\n",
        "  line = clean_doc(review, vocab)\n",
        "# encode and pad review\n",
        "  padded = encode_docs(tokenizer, max_length, [line])\n",
        "# predict sentiment\n",
        "  yhat = model.predict(padded, verbose=0)\n",
        "# retrieve predicted percentage and label\n",
        "  percent_pos = yhat[0,0]\n",
        "  if round(percent_pos) == 0:\n",
        "    return (1-percent_pos), 'NEGATIVE'\n",
        "  return percent_pos, 'POSITIVE'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-iEp-jzMV7M",
        "outputId": "9a9a0c16-03cf-4dd8-b926-145b9c52402c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 85.00\n"
          ]
        }
      ],
      "source": [
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "\n",
        "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
        "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %.2f' % (acc*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu3_4FiJMV94",
        "outputId": "917a70bb-3034-4e3a-8c16-8574a0c9aa7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
            "Sentiment: NEGATIVE (51.207%)\n",
            "Review: [averge . you can watch all by yourself.]\n",
            "Sentiment: NEGATIVE (51.860%)\n"
          ]
        }
      ],
      "source": [
        "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "# test negative text\n",
        "text = 'averge . you can watch all by yourself.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwxLiH9VZxwX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjeCOF8RLGgb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGeHPNY6U8LX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSj4dfhfLHep"
      },
      "source": [
        "# N-gram CNN model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqlKZd3hPF9g"
      },
      "outputs": [],
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "  tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "  re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "  tokens = [re_punc.sub('', w) for w in tokens]\n",
        "# filter out tokens not in vocab\n",
        "  # remove remaining tokens that are not alphabetic\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "  tokens = [word for word in tokens if len(word) > 1]\n",
        "  tokens = ' '.join(tokens)\n",
        "\n",
        "  return tokens\n",
        "\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, is_train):\n",
        "  documents = list()\n",
        "# walk through all files in the folder\n",
        "  for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "    if is_train and filename.startswith('cv9'):\n",
        "      continue\n",
        "    if not is_train and not filename.startswith('cv9'):\n",
        "      continue\n",
        "# create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "# load the doc\n",
        "    doc = load_doc(path)\n",
        "# clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "# add to list\n",
        "    documents.append(tokens)\n",
        "  return documents\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(is_train):\n",
        "# load documents\n",
        "  neg = process_docs('/content/drive/MyDrive/txt_sentoken/neg',  is_train)\n",
        "  pos = process_docs('/content/drive/MyDrive/txt_sentoken/pos',  is_train)\n",
        "  docs = neg + pos\n",
        "# prepare labels\n",
        "  labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "  return docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "# integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(docs)\n",
        "# pad sequences\n",
        "  padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "  return padded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obymEDmKLGjD",
        "outputId": "089f06ef-b542-4b9e-a6d5-d6569e5ee5c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: train.pkl\n",
            "Saved: test.pkl\n"
          ]
        }
      ],
      "source": [
        "from pickle import dump\n",
        "def save_dataset(dataset, filename):\n",
        "  dump(dataset, open(filename, 'wb'))\n",
        "  print('Saved: %s' % filename)\n",
        "# load and clean all reviews\n",
        "train_docs, ytrain = load_clean_dataset(is_train=True)\n",
        "test_docs, ytest = load_clean_dataset(is_train = False)\n",
        "# save training datasets\n",
        "save_dataset([train_docs, ytrain], 'train.pkl')\n",
        "save_dataset([test_docs, ytest], 'test.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssv6rlZGLGlU"
      },
      "outputs": [],
      "source": [
        "from pickle import load\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import concatenate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZi8tOpmLGoU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "  return load(open(filename, 'rb'))\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "  return max([len(s.split()) for s in lines])\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "# integer encode\n",
        "  encoded = tokenizer.texts_to_sequences(lines)\n",
        "# pad encoded sequences\n",
        "  padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "  return padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73aOfBa7LGqr"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "# channel 1\n",
        "  inputs1 = Input(shape=(length,))\n",
        "  embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "  conv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
        "  drop1 = Dropout(0.5)(conv1)\n",
        "  pool1 = MaxPooling1D(pool_size=2)(drop1)\n",
        "  flat1 = Flatten()(pool1)\n",
        "# channel 2\n",
        "  inputs2 = Input(shape=(length,))\n",
        "  embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "  conv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
        "  drop2 = Dropout(0.5)(conv2)\n",
        "  pool2 = MaxPooling1D(pool_size=2)(drop2)\n",
        "  flat2 = Flatten()(pool2)\n",
        "# channel 3\n",
        "  inputs3 = Input(shape=(length,))\n",
        "  embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "  conv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
        "  drop3 = Dropout(0.5)(conv3)\n",
        "  pool3 = MaxPooling1D(pool_size=2)(drop3)\n",
        "  flat3 = Flatten()(pool3)\n",
        "# merge\n",
        "  merged = concatenate([flat1, flat2, flat3])\n",
        "# interpretation\n",
        "  dense1 = Dense(10, activation='relu')(merged)\n",
        "  outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "  model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "# compile\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# summarize\n",
        "  model.summary()\n",
        "  plot_model(model, show_shapes=True, to_file='model.png')\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xQFIRQXLGtM",
        "outputId": "ca7c9de1-7f2c-4c4d-a44b-a84c6b8f69f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max document length: 1380\n",
            "Vocabulary size: 44277\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 1380)]       0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 1380)]       0           []                               \n",
            "                                                                                                  \n",
            " input_9 (InputLayer)           [(None, 1380)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_8 (Embedding)        (None, 1380, 100)    4427700     ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_9 (Embedding)        (None, 1380, 100)    4427700     ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_10 (Embedding)       (None, 1380, 100)    4427700     ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 1377, 32)     12832       ['embedding_8[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 1375, 32)     19232       ['embedding_9[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 1373, 32)     25632       ['embedding_10[0][0]']           \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 1377, 32)     0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 1375, 32)     0           ['conv1d_9[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 1373, 32)     0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_8 (MaxPooling1D)  (None, 688, 32)     0           ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_9 (MaxPooling1D)  (None, 687, 32)     0           ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_10 (MaxPooling1D  (None, 686, 32)     0           ['dropout_8[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " flatten_8 (Flatten)            (None, 22016)        0           ['max_pooling1d_8[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_9 (Flatten)            (None, 21984)        0           ['max_pooling1d_9[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_10 (Flatten)           (None, 21952)        0           ['max_pooling1d_10[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 65952)        0           ['flatten_8[0][0]',              \n",
            "                                                                  'flatten_9[0][0]',              \n",
            "                                                                  'flatten_10[0][0]']             \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 10)           659530      ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 1)            11          ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 14,000,337\n",
            "Trainable params: 14,000,337\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# load training dataset\n",
        "trainLines, trainLabels = load_dataset('train.pkl')\n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "# calculate max document length\n",
        "length = max_length(trainLines)\n",
        "print('Max document length: %d' % length)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "# define model\n",
        "model = define_model(length, vocab_size)\n",
        "# fit model\n",
        "#model.fit([trainX,trainX,trainX], trainLabels, epochs=7, batch_size=16)\n",
        "# save the model\n",
        "#model.save('model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx5Zrmo23mhl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ezc8xD4LGwC",
        "outputId": "92c5dbf9-5192-4527-8b53-741902ef8c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 49.50\n"
          ]
        }
      ],
      "source": [
        "# load training dataset\n",
        "testLines, testLabels = load_dataset('test.pkl')\n",
        "# create tokenizer\n",
        "\n",
        "testX = encode_text(tokenizer, testLines, length)\n",
        "\n",
        "_, acc = model.evaluate([testX,testX,testX], testLabels, verbose=0)\n",
        "print('Test Accuracy: %.2f' % (acc*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_6LGae3cnB8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvtLMMbFiBP5"
      },
      "outputs": [],
      "source": [
        "module_url = 'https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1'\n",
        "embed_size = 128\n",
        "trainable = False\n",
        "hub_layer = hub.KerasLayer(module_url,input_shape=[],output_shape=[embed_size],dtype = tf.string,trainable = trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEIzYdOBVDP0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sltawu8WVDSg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeAhYbBiVFQc"
      },
      "source": [
        "# Character based CNN (LM)(cahracter based neural language modeling\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZLdalEVD5AT",
        "outputId": "74f003dd-082e-49e5-ea20-50530990a237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.23.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n",
            "Installing collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install keras_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8vIobvKsgfV"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras_preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt3MWtG1VDVG"
      },
      "outputs": [],
      "source": [
        "\n",
        "#functions that we are gonna use in our later code\n",
        "\n",
        "def load_doc(filename):\n",
        "  file = open(filename,'r')\n",
        "\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "\n",
        "  return text\n",
        "\n",
        "def save_doc(lines,filename):\n",
        "  data = '\\n'.join(lines)\n",
        "  file = open(filename,'w')\n",
        "  file.write(data)\n",
        "  file.close()\n",
        "\n",
        "\n",
        "\n",
        "def build_model(X):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(75,input_shape= (X.shape[1],X.shape[2])))\n",
        "  model.add(Dense(vocab_size,activation = 'softmax'))\n",
        "\n",
        "  model.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])\n",
        "  model.summary()\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZkmbWE8VDXh"
      },
      "outputs": [],
      "source": [
        "text = load_doc('/content/New Text Document.txt')\n",
        "#print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e05cb_HkVDZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f13007d-be9c-4743-bc25-79376dca60fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "তবুও আমি ভাল থাকি , আকাশ থাকে যেমন, মেঘ ডাকে অজগুবি কে ডাকে অমনে\n"
          ]
        }
      ],
      "source": [
        "tokens = text.split()\n",
        "raw_text = ' '.join(tokens)\n",
        "print(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGXC1nbYVDcw"
      },
      "outputs": [],
      "source": [
        "length = 10\n",
        "sequences = []\n",
        "for i in range(length,len(raw_text)):\n",
        "  seq = raw_text[i-length:i+1]\n",
        "  sequences.append(seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyt_b3zlVDfn"
      },
      "outputs": [],
      "source": [
        "outfile = 'saved_text'\n",
        "save_doc(sequences,outfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9MtPEyMVDiG"
      },
      "outputs": [],
      "source": [
        "#encode input sequence\n",
        "\n",
        "inp_file = '/content/saved_text'\n",
        "data = load_doc(inp_file)\n",
        "lines = data.split('\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgrwf4-iht0p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnysa7UsVDkT"
      },
      "outputs": [],
      "source": [
        "chars = sorted(list(set(data)))\n",
        "mapping = dict((c,i) for i, c in enumerate(chars))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mI8ndmxResTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpgH6udyc1Vu",
        "outputId": "d8b6be33-931f-42fb-9497-c49b863262ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of vacab is 24\n"
          ]
        }
      ],
      "source": [
        "encoded_sequence = []\n",
        "\n",
        "for line in lines:\n",
        "  en_sq = [mapping[char] for char in line]\n",
        "  encoded_sequence.append(en_sq)\n",
        "\n",
        "print('length of vacab is ' + str(len(mapping)) )\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBeuHHT4dNl4"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5oxtXipjDP4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "encoded_sequence = np.array(encoded_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkacL8lVmJbH"
      },
      "outputs": [],
      "source": [
        "X = encoded_sequence[:,:-1]\n",
        "y = encoded_sequence[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMvreMYFmkjm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coIlHk3Xm9KE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "tr_x = np.array([to_categorical(x, num_classes = vocab_size ) for x in X])\n",
        "tr_y = to_categorical(y,num_classes = vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8Wgc6gln823",
        "outputId": "4d607a10-2fd2-414a-a44d-87cec372a160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 75)                30000     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 24)                1824      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,824\n",
            "Trainable params: 31,824\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_model(tr_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz1jkv4ToANP",
        "outputId": "7a2366d1-7305-4ab9-bdbf-357cf27e9c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "2/2 [==============================] - 2s 19ms/step - loss: 3.1555 - accuracy: 0.0556\n",
            "Epoch 2/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3.1374 - accuracy: 0.0926\n",
            "Epoch 3/150\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3.1220 - accuracy: 0.1852\n",
            "Epoch 4/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.1021 - accuracy: 0.2222\n",
            "Epoch 5/150\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 3.0848 - accuracy: 0.2037\n",
            "Epoch 6/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.0650 - accuracy: 0.1852\n",
            "Epoch 7/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.0386 - accuracy: 0.1852\n",
            "Epoch 8/150\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.0093 - accuracy: 0.1852\n",
            "Epoch 9/150\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2.9727 - accuracy: 0.2037\n",
            "Epoch 10/150\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.9290 - accuracy: 0.2037\n",
            "Epoch 11/150\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.8716 - accuracy: 0.2037\n",
            "Epoch 12/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.8049 - accuracy: 0.2037\n",
            "Epoch 13/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.7280 - accuracy: 0.2037\n",
            "Epoch 14/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.6856 - accuracy: 0.2037\n",
            "Epoch 15/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.6713 - accuracy: 0.2037\n",
            "Epoch 16/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.6779 - accuracy: 0.2037\n",
            "Epoch 17/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.6587 - accuracy: 0.2037\n",
            "Epoch 18/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.6266 - accuracy: 0.2037\n",
            "Epoch 19/150\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2.6002 - accuracy: 0.2037\n",
            "Epoch 20/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.5856 - accuracy: 0.2037\n",
            "Epoch 21/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.5759 - accuracy: 0.2037\n",
            "Epoch 22/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.5663 - accuracy: 0.2037\n",
            "Epoch 23/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.5560 - accuracy: 0.2037\n",
            "Epoch 24/150\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2.5420 - accuracy: 0.2037\n",
            "Epoch 25/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.5267 - accuracy: 0.2037\n",
            "Epoch 26/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.5106 - accuracy: 0.2037\n",
            "Epoch 27/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.4977 - accuracy: 0.2037\n",
            "Epoch 28/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2.4828 - accuracy: 0.2037\n",
            "Epoch 29/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.4664 - accuracy: 0.2037\n",
            "Epoch 30/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.4552 - accuracy: 0.2037\n",
            "Epoch 31/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.4321 - accuracy: 0.2037\n",
            "Epoch 32/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.4105 - accuracy: 0.2222\n",
            "Epoch 33/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.3919 - accuracy: 0.2407\n",
            "Epoch 34/150\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2.3693 - accuracy: 0.2778\n",
            "Epoch 35/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.3446 - accuracy: 0.3333\n",
            "Epoch 36/150\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2.3196 - accuracy: 0.3704\n",
            "Epoch 37/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.2973 - accuracy: 0.3889\n",
            "Epoch 38/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.2678 - accuracy: 0.3889\n",
            "Epoch 39/150\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2.2403 - accuracy: 0.3519\n",
            "Epoch 40/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.2158 - accuracy: 0.3333\n",
            "Epoch 41/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 2.1938 - accuracy: 0.3333\n",
            "Epoch 42/150\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2.1708 - accuracy: 0.3148\n",
            "Epoch 43/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.1457 - accuracy: 0.3148\n",
            "Epoch 44/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.1236 - accuracy: 0.3704\n",
            "Epoch 45/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.0926 - accuracy: 0.3704\n",
            "Epoch 46/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.0714 - accuracy: 0.3704\n",
            "Epoch 47/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.0416 - accuracy: 0.3889\n",
            "Epoch 48/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.0149 - accuracy: 0.3889\n",
            "Epoch 49/150\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.9876 - accuracy: 0.3704\n",
            "Epoch 50/150\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.9626 - accuracy: 0.3519\n",
            "Epoch 51/150\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.9386 - accuracy: 0.3704\n",
            "Epoch 52/150\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1.9065 - accuracy: 0.4444\n",
            "Epoch 53/150\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.8929 - accuracy: 0.4259\n",
            "Epoch 54/150\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.8601 - accuracy: 0.4444\n",
            "Epoch 55/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.8339 - accuracy: 0.5000\n",
            "Epoch 56/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.8025 - accuracy: 0.5370\n",
            "Epoch 57/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.7732 - accuracy: 0.5556\n",
            "Epoch 58/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.7467 - accuracy: 0.5185\n",
            "Epoch 59/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.7142 - accuracy: 0.5185\n",
            "Epoch 60/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.6979 - accuracy: 0.5000\n",
            "Epoch 61/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.6606 - accuracy: 0.5370\n",
            "Epoch 62/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.6363 - accuracy: 0.5741\n",
            "Epoch 63/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.6061 - accuracy: 0.5741\n",
            "Epoch 64/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.5755 - accuracy: 0.5556\n",
            "Epoch 65/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.5486 - accuracy: 0.5556\n",
            "Epoch 66/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.5092 - accuracy: 0.5741\n",
            "Epoch 67/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.4801 - accuracy: 0.5741\n",
            "Epoch 68/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.4450 - accuracy: 0.5741\n",
            "Epoch 69/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.4155 - accuracy: 0.5741\n",
            "Epoch 70/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.3854 - accuracy: 0.6111\n",
            "Epoch 71/150\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.3517 - accuracy: 0.6111\n",
            "Epoch 72/150\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.3263 - accuracy: 0.6667\n",
            "Epoch 73/150\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.2925 - accuracy: 0.6852\n",
            "Epoch 74/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2608 - accuracy: 0.7037\n",
            "Epoch 75/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2244 - accuracy: 0.6852\n",
            "Epoch 76/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.1922 - accuracy: 0.7037\n",
            "Epoch 77/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.1608 - accuracy: 0.7222\n",
            "Epoch 78/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.1319 - accuracy: 0.7593\n",
            "Epoch 79/150\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.1036 - accuracy: 0.8148\n",
            "Epoch 80/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.0731 - accuracy: 0.8333\n",
            "Epoch 81/150\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.0401 - accuracy: 0.8148\n",
            "Epoch 82/150\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1.0174 - accuracy: 0.8333\n",
            "Epoch 83/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.9834 - accuracy: 0.8333\n",
            "Epoch 84/150\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.9625 - accuracy: 0.8148\n",
            "Epoch 85/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.9321 - accuracy: 0.8889\n",
            "Epoch 86/150\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.9148 - accuracy: 0.8704\n",
            "Epoch 87/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.8808 - accuracy: 0.8704\n",
            "Epoch 88/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.8516 - accuracy: 0.8889\n",
            "Epoch 89/150\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.8474 - accuracy: 0.8889\n",
            "Epoch 90/150\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.8323 - accuracy: 0.8889\n",
            "Epoch 91/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.7920 - accuracy: 0.9259\n",
            "Epoch 92/150\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.7915 - accuracy: 0.8889\n",
            "Epoch 93/150\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.7613 - accuracy: 0.8889\n",
            "Epoch 94/150\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.7261 - accuracy: 0.9259\n",
            "Epoch 95/150\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.7094 - accuracy: 0.9444\n",
            "Epoch 96/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.6962 - accuracy: 0.9259\n",
            "Epoch 97/150\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.6746 - accuracy: 0.9444\n",
            "Epoch 98/150\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6533 - accuracy: 0.9259\n",
            "Epoch 99/150\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.6275 - accuracy: 0.9074\n",
            "Epoch 100/150\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.6009 - accuracy: 0.9444\n",
            "Epoch 101/150\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5895 - accuracy: 0.9444\n",
            "Epoch 102/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5687 - accuracy: 0.9630\n",
            "Epoch 103/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5568 - accuracy: 0.9630\n",
            "Epoch 104/150\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5469 - accuracy: 0.9630\n",
            "Epoch 105/150\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5221 - accuracy: 0.9630\n",
            "Epoch 106/150\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5016 - accuracy: 0.9630\n",
            "Epoch 107/150\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4998 - accuracy: 0.9630\n",
            "Epoch 108/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4737 - accuracy: 0.9630\n",
            "Epoch 109/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4653 - accuracy: 0.9630\n",
            "Epoch 110/150\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4494 - accuracy: 0.9630\n",
            "Epoch 111/150\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.4339 - accuracy: 0.9815\n",
            "Epoch 112/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4203 - accuracy: 0.9815\n",
            "Epoch 113/150\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.4099 - accuracy: 0.9815\n",
            "Epoch 114/150\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 0.3953 - accuracy: 0.9815\n",
            "Epoch 115/150\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3829 - accuracy: 0.9815\n",
            "Epoch 116/150\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.3757 - accuracy: 0.9815\n",
            "Epoch 117/150\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.3638 - accuracy: 0.9815\n",
            "Epoch 118/150\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.3573 - accuracy: 0.9815\n",
            "Epoch 119/150\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.3434 - accuracy: 0.9815\n",
            "Epoch 120/150\n",
            "2/2 [==============================] - 0s 59ms/step - loss: 0.3309 - accuracy: 0.9815\n",
            "Epoch 121/150\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.3226 - accuracy: 0.9815\n",
            "Epoch 122/150\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.3125 - accuracy: 1.0000\n",
            "Epoch 123/150\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.3041 - accuracy: 1.0000\n",
            "Epoch 124/150\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 0.2908 - accuracy: 1.0000\n",
            "Epoch 125/150\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.2848 - accuracy: 1.0000\n",
            "Epoch 126/150\n",
            "2/2 [==============================] - 0s 46ms/step - loss: 0.2803 - accuracy: 1.0000\n",
            "Epoch 127/150\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.2699 - accuracy: 1.0000\n",
            "Epoch 128/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2607 - accuracy: 0.9815\n",
            "Epoch 129/150\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2539 - accuracy: 1.0000\n",
            "Epoch 130/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2437 - accuracy: 1.0000\n",
            "Epoch 131/150\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.2360 - accuracy: 1.0000\n",
            "Epoch 132/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2300 - accuracy: 1.0000\n",
            "Epoch 133/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2222 - accuracy: 1.0000\n",
            "Epoch 134/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2162 - accuracy: 1.0000\n",
            "Epoch 135/150\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.2088 - accuracy: 1.0000\n",
            "Epoch 136/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.2076 - accuracy: 1.0000\n",
            "Epoch 137/150\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.1956 - accuracy: 1.0000\n",
            "Epoch 138/150\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1937 - accuracy: 1.0000\n",
            "Epoch 139/150\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.1840 - accuracy: 1.0000\n",
            "Epoch 140/150\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.1828 - accuracy: 1.0000\n",
            "Epoch 141/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1763 - accuracy: 1.0000\n",
            "Epoch 142/150\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.1712 - accuracy: 1.0000\n",
            "Epoch 143/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1647 - accuracy: 1.0000\n",
            "Epoch 144/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1620 - accuracy: 1.0000\n",
            "Epoch 145/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1567 - accuracy: 1.0000\n",
            "Epoch 146/150\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.1527 - accuracy: 1.0000\n",
            "Epoch 147/150\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1477 - accuracy: 1.0000\n",
            "Epoch 148/150\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.1455 - accuracy: 1.0000\n",
            "Epoch 149/150\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.1414 - accuracy: 1.0000\n",
            "Epoch 150/150\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.1380 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x79dabcbf1a50>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "model.fit(tr_x,tr_y,epochs = 150)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-EwohQXoEUX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B94kgna0oKav"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FwDNJz_oUTs"
      },
      "outputs": [],
      "source": [
        "def generate_seq(model , mapping , seq_length , seed, n_chars):\n",
        "  import numpy as np\n",
        "  in_text = seed\n",
        "  for _ in range(n_chars):\n",
        "# encode the characters as integers\n",
        "    encoded = [mapping[char] for char in in_text]\n",
        "# truncate sequences to a fixed length\n",
        "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "# one hot encode\n",
        "    encoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "    #encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
        "# predict character\n",
        "    yhat = model.predict(encoded)\n",
        "\n",
        "    classes_x=np.argmax(yhat,axis=1)\n",
        "\n",
        "    out_char = ''\n",
        "\n",
        "    for char , index in mapping.items():\n",
        "      if index == classes_x:\n",
        "        out_char = char\n",
        "        break\n",
        "\n",
        "    in_text += out_char\n",
        "\n",
        "  return in_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNUxLDiPCq0e",
        "outputId": "eba1493d-462c-4aca-cfae-921e8517a053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "যেমনে  \n"
          ]
        }
      ],
      "source": [
        "print(generate_seq(model, mapping, 10, 'যেমন', 3))\n",
        "# test mid-line\n",
        "#print(generate_seq(model, mapping, 10, 'king was i', 20))\n",
        "# test not in original\n",
        "#print(generate_seq(model, mapping, 10, 'hello worl', 20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7pAFRFFC3Ho",
        "outputId": "cb9a2cbb-5098-41c8-bb71-a817f9b77a11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 80ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "sing a song of sixpence, A poc\n"
          ]
        }
      ],
      "source": [
        "print(generate_seq(model, mapping, 10, 'sing a son', 20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7XeGBxMF_bn",
        "outputId": "10a44e07-e293-48b5-81ca-1cf30941658d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "king was in his counting house\n"
          ]
        }
      ],
      "source": [
        "print(generate_seq(model, mapping, 10, 'king was i', 20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP9m61ayGC3z",
        "outputId": "2def6785-17c5-4970-d30d-b1f3bd7300ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 71ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "i lo ke pe. Foffo  theepi\n"
          ]
        }
      ],
      "source": [
        "print(generate_seq(model, mapping, 10, 'i lo ', 20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJp67Gv5I3dH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}